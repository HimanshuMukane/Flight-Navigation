{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: [Errno 2] No such file or directory: 'path_to_your_service_account.json'\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import google.auth\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.service_account import Credentials\n",
    "\n",
    "def scrape_page(url):\n",
    "    # Initialize the Chrome driver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    try:\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Locate the table within the specified div\n",
    "        table_div = soup.find('div', class_='col-lg-12 largetable')\n",
    "        table = table_div.find('table') if table_div else None\n",
    "        \n",
    "        extracted_data = []\n",
    "\n",
    "        if table:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                row_data = []\n",
    "                headers = row.find_all('th')\n",
    "                for header in headers:\n",
    "                    row_data.append(header.get_text().strip())\n",
    "                cells = row.find_all('td')\n",
    "                for cell in cells:\n",
    "                    row_data.append(cell.get_text().strip())\n",
    "                extracted_data.append(row_data)\n",
    "        \n",
    "        # Write the extracted data to Google Sheet\n",
    "        write_to_google_sheet(extracted_data)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error:\", e)\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def write_to_google_sheet(data):\n",
    "    # Define the scope and authenticate with the service account\n",
    "    SCOPES = ['https://www.googleapis.com/auth/spreadsheets']\n",
    "    SERVICE_ACCOUNT_FILE = 'path_to_your_service_account.json'\n",
    "    \n",
    "    credentials = Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES)\n",
    "    \n",
    "    # The ID of the spreadsheet\n",
    "    SPREADSHEET_ID = '1b204kgy52jsyu4ptvFO1Ts9yCbDtwtq6AxoTIjCmbtY'\n",
    "\n",
    "    # Build the service\n",
    "    service = build('sheets', 'v4', credentials=credentials)\n",
    "    \n",
    "    # Prepare the data to be inserted\n",
    "    body = {\n",
    "        'values': data\n",
    "    }\n",
    "\n",
    "    # Clear the existing content\n",
    "    clear_range = 'Sheet1!A:Z'  # Adjust the range as needed\n",
    "    service.spreadsheets().values().clear(spreadsheetId=SPREADSHEET_ID, range=clear_range).execute()\n",
    "    \n",
    "    # Insert the data into the Google Sheet\n",
    "    result = service.spreadsheets().values().update(\n",
    "        spreadsheetId=SPREADSHEET_ID, range='Sheet1!A1',\n",
    "        valueInputOption='RAW', body=body).execute()\n",
    "    \n",
    "    print(f\"{result.get('updatedCells')} cells updated.\")\n",
    "\n",
    "# Replace the URL with the target URL\n",
    "url = 'https://www.bseindia.com/corporates/ann.html'\n",
    "scrape_page(url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
